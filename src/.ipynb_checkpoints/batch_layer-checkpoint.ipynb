{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94110d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker exec -it spark2 bash\n",
    "\n",
    "# export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH\n",
    "# jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "\n",
    "#http://127.0.0.1:9888/lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49c88ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-b91c906f-fc1d-4dc3-a5d7-084b018b49e6;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.4.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.3.2 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.9.1 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.6 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector_2.12;3.4.0 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 in central\n",
      "\tfound com.datastax.oss#java-driver-core-shaded;4.13.0 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.5.0 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;4.1.18 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound com.github.spotbugs#spotbugs-annotations;3.1.12 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central\n",
      "\tfound com.datastax.oss#java-driver-query-builder;4.13.0 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.10 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.11 in central\n",
      ":: resolution report :: resolve 1221ms :: artifacts dl 37ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.5.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.12;3.4.0 from central in [default]\n",
      "\tcom.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.18 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.3.2 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.4.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.0 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.11 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.6 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.9.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 by [com.google.code.findbugs#jsr305;3.0.2] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 by [org.slf4j#slf4j-api;2.0.6] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   29  |   0   |   0   |   2   ||   27  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-b91c906f-fc1d-4dc3-a5d7-084b018b49e6\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 27 already retrieved (0kB/25ms)\n",
      "25/10/09 10:47:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/09 10:47:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, desc, row_number, when, window, current_timestamp, collect_set, round, concat_ws\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Retailrocket-BatchLayer\") \\\n",
    "    .master(\"spark://spark2:7077\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.driver.cores\", \"1\") \\\n",
    "    .config(\"spark.cores.max\", \"1\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", \"cassandra1\") \\\n",
    "    .config(\"spark.cassandra.connection.port\", \"9042\") \\\n",
    "    .config(\"spark.jars.packages\",\n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0,\"\n",
    "            \"com.datastax.spark:spark-cassandra-connector_2.12:3.4.0\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf317188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"hdfs://hadoop:8020/data/retailrocket_raw\")\n",
    "# df.createOrReplaceTempView(\"batch_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ceefaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "\n",
    "cluster = Cluster(['cassandra1'], port=9042)\n",
    "session = cluster.connect()\n",
    "\n",
    "# Creating a keyspace named event_data_view, where data will be written for kappa\n",
    "session.execute(\n",
    "    \"\"\"\n",
    "    CREATE KEYSPACE IF NOT EXISTS event_data_view\n",
    "    WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'};\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "session.set_keyspace(\"event_data_view\")\n",
    "\n",
    "\n",
    "session.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS top10_items (\n",
    "        event text,\n",
    "        itemid text,\n",
    "        total_count int,\n",
    "        rank int,\n",
    "        PRIMARY KEY (event, rank, itemid)\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "session.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS top_abandoned_items (\n",
    "        itemid text,\n",
    "        addtocart int,\n",
    "        transaction int,\n",
    "        abandoned_count int,\n",
    "        abandon_rate double,\n",
    "        PRIMARY KEY (itemid)\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "session.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS event_counts_30s (\n",
    "        event text,\n",
    "        window_start timestamp,\n",
    "        window_end timestamp,\n",
    "        count int,\n",
    "        PRIMARY KEY (event, window_start)\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "session.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS conversion_rate (\n",
    "        batch_timestamp timestamp PRIMARY KEY,\n",
    "        n_view bigint,\n",
    "        addtocart bigint,\n",
    "        transaction bigint,\n",
    "        view_to_cart_rate double,\n",
    "        cart_to_transaction_rate double,\n",
    "        view_to_transaction_rate double\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "session.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS frequent_itemsets (\n",
    "        items text,\n",
    "        freq int,\n",
    "        PRIMARY KEY (items)\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "session.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS association_rules (\n",
    "        antecedent text,\n",
    "        consequent text,\n",
    "        confidence double,\n",
    "        lift double,\n",
    "        PRIMARY KEY (antecedent, consequent)\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# CREATE TABLE IF NOT EXISTS association_rules (\n",
    "#         antecedent text,   -- sản phẩm(s) ban đầu\n",
    "#         consequent text,   -- sản phẩm(s) thường đi kèm\n",
    "#         confidence double,       -- độ tin cậy\n",
    "#         lift double,             -- độ nâng\n",
    "#         PRIMARY KEY (antecedent, consequent)\n",
    "#     )\n",
    "\n",
    "session.shutdown()\n",
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "461e546d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-----------+----+\n",
      "|itemid|    event|total_count|rank|\n",
      "+------+---------+-----------+----+\n",
      "|431417|addtocart|          5|   1|\n",
      "|244506|addtocart|          4|   2|\n",
      "|   546|addtocart|          4|   3|\n",
      "|355741|addtocart|          3|   4|\n",
      "|269610|addtocart|          3|   5|\n",
      "+------+---------+-----------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 3️⃣ Tính tổng số lượt cho mỗi (itemid, event)\n",
    "agg = df.groupBy(\"itemid\", \"event\").agg(count(\"*\").alias(\"total_count\"))\n",
    "\n",
    "# 4️⃣ Tạo ranking theo từng event\n",
    "window_spec = Window.partitionBy(\"event\").orderBy(desc(\"total_count\"))\n",
    "ranked = agg.withColumn(\"rank\", row_number().over(window_spec))\n",
    "\n",
    "# 5️⃣ Lọc ra Top 10 sản phẩm cho mỗi loại event\n",
    "top10 = ranked.filter(col(\"rank\") <= 10)\n",
    "\n",
    "top10.show(5)\n",
    "\n",
    "# 6️⃣ Ghi vào Cassandra\n",
    "top10.write \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"confirm.truncate\", \"true\") \\\n",
    "    .options(table=\"top10_items\", keyspace=\"event_data_view\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a816b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 3️⃣ Đếm số lượt addtocart và transaction cho từng itemid\n",
    "agg = df.groupBy(\"itemid\").pivot(\"event\", [\"addtocart\", \"transaction\"]).agg(count(\"*\"))\n",
    "\n",
    "# 4️⃣ Thay null bằng 0\n",
    "agg = agg.fillna(0, subset=[\"addtocart\", \"transaction\"])\n",
    "\n",
    "# 5️⃣ Tính số lượt bị bỏ giỏ và tỷ lệ bỏ giỏ\n",
    "agg = agg.withColumn(\"abandoned_count\", col(\"addtocart\") - col(\"transaction\")) \\\n",
    "         .withColumn(\"abandon_rate\", \n",
    "                     when(col(\"addtocart\") > 0, \n",
    "                          (col(\"addtocart\") - col(\"transaction\")) / col(\"addtocart\"))\n",
    "                     .otherwise(0))\n",
    "\n",
    "# 6️⃣ Lấy top 10 sản phẩm có tỷ lệ bỏ giỏ cao nhất\n",
    "top_abandoned = agg.filter(col(\"addtocart\") > 0) \\\n",
    "                   .orderBy(desc(\"abandon_rate\"), desc(\"abandoned_count\")) \\\n",
    "                   .limit(10)\n",
    "\n",
    "# 7️⃣ Ghi kết quả vào Cassandra\n",
    "top_abandoned.write \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"confirm.truncate\", \"true\") \\\n",
    "    .options(table=\"top_abandoned_items\", keyspace=\"event_data_view\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0a8a42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2️⃣ Chuyển timestamp (ms) -> timestamp type\n",
    "# df = df.withColumn(\"event_ts\", (col(\"timestamp\") / 1000).cast(\"timestamp\"))\n",
    "\n",
    "# # 3️⃣ Gom nhóm theo cửa sổ 10 giây\n",
    "# agg_10s = df.groupBy(\n",
    "#     window(col(\"event_ts\"), \"30 seconds\"),\n",
    "#     col(\"event\")\n",
    "# ).agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "# # 4️⃣ Tách cột window thành start / end để lưu\n",
    "# agg_10s = agg_10s.select(\n",
    "#     col(\"window.start\").alias(\"window_start\"),\n",
    "#     col(\"window.end\").alias(\"window_end\"),\n",
    "#     col(\"event\"),\n",
    "#     col(\"count\")\n",
    "# )\n",
    "\n",
    "# # 5️⃣ Ghi vào Cassandra\n",
    "# agg_10s.write \\\n",
    "#     .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "#     .mode(\"append\") \\\n",
    "#     .options(table=\"event_counts_30s\", keyspace=\"event_data_view\") \\\n",
    "#     .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caa21f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-----------+-----------------+------------------------+------------------------+--------------------+\n",
      "|n_view|addtocart|transaction|view_to_cart_rate|cart_to_transaction_rate|view_to_transaction_rate|     batch_timestamp|\n",
      "+------+---------+-----------+-----------------+------------------------+------------------------+--------------------+\n",
      "|   610|      296|        224|            48.52|                   75.68|                   36.72|2025-10-09 10:35:...|\n",
      "+------+---------+-----------+-----------------+------------------------+------------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 2️⃣ Đếm số lượng từng loại event\n",
    "event_counts = df.groupBy(\"event\").agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "# 3️⃣ Chuyển thành bảng pivot để có cột view/addtocart/transaction\n",
    "pivot = event_counts.groupBy().pivot(\"event\", [\"view\", \"addtocart\", \"transaction\"]).sum(\"count\")\n",
    "pivot = pivot.na.fill(0, [\"view\", \"addtocart\", \"transaction\"]) \n",
    "\n",
    "# 4️⃣ Tính tỷ lệ chuyển đổi\n",
    "conversion = pivot.withColumn(\n",
    "        \"view_to_cart_rate\",\n",
    "        round(when(col(\"view\") > 0, col(\"addtocart\") / col(\"view\") * 100).otherwise(0), 2)\n",
    "    ).withColumn(\n",
    "        \"cart_to_transaction_rate\",\n",
    "        round(when(col(\"addtocart\") > 0, col(\"transaction\") / col(\"addtocart\") * 100).otherwise(0), 2)\n",
    "    ).withColumn(\n",
    "        \"view_to_transaction_rate\",\n",
    "        round(when(col(\"view\") > 0, col(\"transaction\") / col(\"view\") * 100).otherwise(0), 2)\n",
    "    ).withColumn(\n",
    "        \"batch_timestamp\", current_timestamp()\n",
    "    ).withColumnRenamed(\"view\", \"n_view\")\n",
    "\n",
    "conversion.show()\n",
    "\n",
    "conversion.write \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"confirm.truncate\", \"true\") \\\n",
    "    .options(table=\"conversion_rate\", keyspace=\"event_data_view\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16ee3d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|               items|freq|\n",
      "+--------------------+----+\n",
      "|            [463663]|   1|\n",
      "|            [333140]|   1|\n",
      "|    [333140, 275692]|   1|\n",
      "|            [150215]|   1|\n",
      "|            [192990]|   1|\n",
      "|    [192990, 241716]|   1|\n",
      "|            [210087]|   1|\n",
      "|    [210087, 333140]|   1|\n",
      "|[210087, 333140, ...|   1|\n",
      "|    [210087, 275692]|   1|\n",
      "+--------------------+----+\n",
      "\n",
      "+--------------------+----------+----------+----+\n",
      "|          antecedent|consequent|confidence|lift|\n",
      "+--------------------+----------+----------+----+\n",
      "|    [212006, 235880]|  [399192]|       1.0|58.0|\n",
      "|    [212006, 235880]|   [41963]|       1.0|58.0|\n",
      "|    [212006, 235880]|     [720]|       1.0|58.0|\n",
      "|    [212006, 235880]|  [450641]|       1.0|58.0|\n",
      "|    [212006, 235880]|  [124214]|       1.0|58.0|\n",
      "|    [212006, 235880]|  [368403]|       1.0|58.0|\n",
      "|     [248455, 89688]|   [22568]|       1.0|58.0|\n",
      "|[399192, 41963, 3...|  [450641]|       1.0|58.0|\n",
      "|[399192, 41963, 3...|     [720]|       1.0|58.0|\n",
      "|    [138827, 333140]|  [210087]|       1.0|58.0|\n",
      "+--------------------+----------+----------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.fpm import FPGrowth\n",
    "\n",
    "\n",
    "# 3️⃣ Lọc ra các sự kiện mua hàng (hoặc addtocart nếu bạn muốn mở rộng hành vi)\n",
    "transactions = df.filter(df.event.isin(\"transaction\")).limit(100)\n",
    "\n",
    "# 4️⃣ Gom nhóm theo visitorid -> danh sách sản phẩm người đó đã mua/thêm giỏ\n",
    "baskets = transactions.groupBy(\"visitorid\") \\\n",
    "    .agg(collect_set(\"itemid\").alias(\"items\"))\n",
    "\n",
    "# 5️⃣ Áp dụng FP-Growth để tìm liên kết sản phẩm\n",
    "fpGrowth = FPGrowth(itemsCol=\"items\", minSupport=0.005, minConfidence=0.2)\n",
    "model = fpGrowth.fit(baskets)\n",
    "\n",
    "# 6️⃣ Các tập sản phẩm thường xuất hiện cùng nhau\n",
    "freqItemsets = model.freqItemsets.limit(10)\n",
    "freqItemsets.show(10)\n",
    "\n",
    "# 7️⃣ Luật liên kết giữa các sản phẩm\n",
    "rules = model.associationRules.drop(\"support\").limit(10)\n",
    "rules.show(10)\n",
    "\n",
    "# 9️⃣ Ghi kết quả vào Cassandra\n",
    "freqItemsets.write \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"confirm.truncate\", \"true\") \\\n",
    "    .options(table=\"frequent_itemsets\", keyspace=\"event_data_view\") \\\n",
    "    .save()\n",
    "\n",
    "rules.write \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"confirm.truncate\", \"true\") \\\n",
    "    .options(table=\"association_rules\", keyspace=\"event_data_view\") \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f11bd13-2a99-4fb9-8c95-180d4cd2544f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/09 10:49:17 WARN ChannelPool: [s0|cassandra1/172.18.0.7:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=f1c9c688-5db3-4b64-bd62-61c825dc2733, APPLICATION_NAME=Spark-Cassandra-Connector-app-20251009104735-0007}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n",
      "25/10/09 10:49:17 WARN ControlConnection: [s0] Error connecting to Node(endPoint=cassandra1/172.18.0.7:9042, hostId=4f4074f1-c47c-4b1c-a48f-747120bea1ea, hashCode=44f0f50e), trying next node (ConnectionInitException: [s0|control|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=f1c9c688-5db3-4b64-bd62-61c825dc2733, APPLICATION_NAME=Spark-Cassandra-Connector-app-20251009104735-0007}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2341a5ae-15ec-4a4e-ae22-af800e121103",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_items = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        itemid,\n",
    "        COUNT(*) AS transaction_count\n",
    "    FROM batch_view\n",
    "    WHERE event = 'view'\n",
    "    GROUP BY itemid\n",
    "    ORDER BY transaction_count DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "top_items.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9248fd0-6e3c-43e6-b93b-1ea29ea439e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
